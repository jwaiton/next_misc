{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,os.path\n",
    "sys.path.append(\"../../../\") # if you move files around, you need to adjust this!\n",
    "sys.path.append(os.path.expanduser('~/code/eol_hsrl_python'))\n",
    "os.environ['ICTDIR']='/home/e78368jw/Documents/NEXT_CODE/IC'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import tables as tb\n",
    "import IC.invisible_cities.io.dst_io                           as     dstio\n",
    "import IC.invisible_cities.io.mcinfo_io as mcio\n",
    "from    IC.invisible_cities.core.core_functions   import shift_to_bin_centers\n",
    "\n",
    "import scipy.special as special\n",
    "from scipy.stats import skewnorm\n",
    "from scipy.optimize import curve_fit\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# timekeeping\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "# the functions\n",
    "import core.functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(folder_path):\n",
    "    '''\n",
    "    Load in multiple h5 files and produce dataframes corresponding to /Tracking/Tracks, /MC/Particles, and their corresponding\n",
    "    eventmap.\n",
    "\n",
    "    Args:\n",
    "        folder_path     :       path to folder of h5 files\n",
    "    Returns:\n",
    "        (tracks,        :       tracks dataframe\n",
    "        particles,      :       MC particle information dataframe\n",
    "        eventmap)       :       eventmap for MC -> Tracks\n",
    "    '''\n",
    "    file_names = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f)) and f.endswith('.h5')]\n",
    "    \n",
    "    # remove any files that dont end in h5\n",
    "\n",
    "    # NOTE Break this section up, its annoying like this.\n",
    "    dfs = []\n",
    "    df_trs = []\n",
    "    df_ems = []\n",
    "    i = 0\n",
    "    end = len(file_names)\n",
    "    # create massive dataframe with all of them\n",
    "    for file in file_names:\n",
    "        file_path = folder_path + file\n",
    "        df = dstio.load_dst(file_path, 'Tracking', 'Tracks')\n",
    "        dfs.append(df)\n",
    "        # include MC particles (boooo takes ages)\n",
    "\n",
    "        # collecting the correct components of the file, not exactly sure how this works\n",
    "        df_ps = pd.read_hdf(file_path, 'MC/particles')\n",
    "        #df_ps = df_ps[df_ps.creator_proc == 'conv']\n",
    "        # collecting event map\n",
    "        df_em = mcio.load_eventnumbermap(file_path).set_index('nexus_evt')\n",
    "        df_trs.append(df_ps)\n",
    "        df_ems.append(df_em)\n",
    "        i += 1\n",
    "\n",
    "        if (i%50 == 0):\n",
    "            print(i)\n",
    "\n",
    "    tracks = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "    particles = pd.concat(df_trs, ignore_index=True)\n",
    "    particles['event_id'] = particles['event_id'] * 2   # double it\n",
    "\n",
    "    eventmap = pd.concat([dt for dt in df_ems])\n",
    "    # create particle list also\n",
    "\n",
    "    return (tracks, particles, eventmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_speed(fun):\n",
    "\n",
    "    # collect file path\n",
    "    initi = 'isaura_sample/'\n",
    "    folder_path = ['isaura_sample_5/', 'isaura_sample_20/', 'isaura_sample_50/', 'isaura_sample_100/']\n",
    "    file_no = ['5', '20', '50', '100']\n",
    "\n",
    "\n",
    "    print(f\"Function: {fun.__name__}\")\n",
    "\n",
    "    for i in range(len(folder_path)):\n",
    "        print(f\"{file_no[i]} files:\")\n",
    "        start = time.time()\n",
    "        data = fun(initi + folder_path[i])\n",
    "        end = time.time()\n",
    "        print(f'{end-start:.4f} s')\n",
    "        print(\"\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def compare_data(data_1, data_2):\n",
    "    print(f'Are dataframes equivalent?\\n{not (((data_1 == data_2) == False).values.any())}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: load_data\n",
      "5 files:\n",
      "0.7353 s\n",
      "\n",
      "20 files:\n",
      "2.9814 s\n",
      "\n",
      "50 files:\n",
      "50\n",
      "7.0594 s\n",
      "\n",
      "100 files:\n",
      "50\n",
      "100\n",
      "14.8477 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = test_speed(load_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def load_single_file(file_path):\n",
    "    \"\"\" Helper function to load data from a single file. \"\"\"\n",
    "    tracks_df = dstio.load_dst(file_path, 'Tracking', 'Tracks')\n",
    "    particles_df = pd.read_hdf(file_path, 'MC/particles')\n",
    "    eventmap_df = mcio.load_eventnumbermap(file_path).set_index('nexus_evt')\n",
    "    \n",
    "    # Modify particles data\n",
    "    particles_df['event_id'] = particles_df['event_id'] * 2\n",
    "    \n",
    "    return tracks_df, particles_df, eventmap_df\n",
    "\n",
    "def load_data_new(folder_path):\n",
    "    '''\n",
    "    Load in multiple h5 files and produce dataframes corresponding to /Tracking/Tracks, /MC/Particles, and their corresponding\n",
    "    eventmap.\n",
    "    '''\n",
    "    file_names = [f for f in os.listdir(folder_path) if f.endswith('.h5')]\n",
    "    file_paths = [os.path.join(folder_path, f) for f in file_names]\n",
    "\n",
    "    # Use ProcessPoolExecutor to parallelize the data loading process\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        results = list(executor.map(load_single_file, file_paths))\n",
    "    \n",
    "    # Separate the results into respective lists\n",
    "    tracks_list, particles_list, eventmap_list = zip(*results)\n",
    "\n",
    "    # Concatenate all the dataframes at once\n",
    "    tracks = pd.concat(tracks_list, axis=0, ignore_index=True)\n",
    "    particles = pd.concat(particles_list, ignore_index=True)\n",
    "    eventmap = pd.concat(eventmap_list, ignore_index=True)\n",
    "\n",
    "    return tracks, particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: load_data_new\n",
      "5 files:\n",
      "1.5540 s\n",
      "\n",
      "20 files:\n",
      "2.6461 s\n",
      "\n",
      "50 files:\n",
      "5.1486 s\n",
      "\n",
      "100 files:\n",
      "9.5462 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_new = test_speed(load_data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are dataframes equivalent?\n",
      "True\n",
      "Are dataframes equivalent?\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "compare_data(data[0], data_new[0])\n",
    "compare_data(data[1], data_new[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trying a different shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_file_loop(file_path):\n",
    "    \"\"\"Helper function to load data from a single file.\"\"\"\n",
    "    tracks_df = dstio.load_dst(file_path, 'Tracking', 'Tracks')\n",
    "    particles_df = pd.read_hdf(file_path, 'MC/particles')\n",
    "    eventmap_df = mcio.load_eventnumbermap(file_path)\n",
    "\n",
    "    # Ensure the eventmap has a consistent index\n",
    "    eventmap_df = eventmap_df.set_index('nexus_evt')\n",
    "\n",
    "    # Modify particles data (example operation)\n",
    "    particles_df['event_id'] = particles_df['event_id'] * 2\n",
    "    \n",
    "    return tracks_df, particles_df, eventmap_df\n",
    "\n",
    "def load_data_loop(folder_path):\n",
    "    '''\n",
    "    Load multiple h5 files and produce dataframes corresponding to /Tracking/Tracks, /MC/Particles, and their corresponding\n",
    "    eventmap.\n",
    "    '''\n",
    "    file_names = [f for f in os.listdir(folder_path) if f.endswith('.h5')]\n",
    "    file_paths = [os.path.join(folder_path, f) for f in file_names]\n",
    "\n",
    "    # Initialize empty DataFrames\n",
    "    tracks = pd.DataFrame()\n",
    "    particles = pd.DataFrame()\n",
    "    eventmap = pd.DataFrame()\n",
    "\n",
    "    # Loop through files and append data to the master DataFrames\n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        tracks_df, particles_df, eventmap_df = load_single_file(file_path)\n",
    "        \n",
    "        # Append data to master DataFrames\n",
    "        tracks = tracks.append(tracks_df, ignore_index=True)\n",
    "        particles = particles.append(particles_df, ignore_index=True)\n",
    "        eventmap = eventmap.append(eventmap_df.reset_index(), ignore_index=True)\n",
    "\n",
    "        if (i+1) % 50 == 0:\n",
    "            print(f\"{i+1} files processed\")\n",
    "\n",
    "    return tracks, particles, eventmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_newest = test_speed(load_data_loop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IC-3.8-2022-04-13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
