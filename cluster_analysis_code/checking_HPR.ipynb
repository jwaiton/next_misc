{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a17e81-16c7-4c2c-9d80-b62d876d128b",
   "metadata": {},
   "source": [
    "### Checking HPR\n",
    "\n",
    "Looking at the high pressure runs, see how they work through the cuts, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40d8b4dc-a045-432f-ac50-4b0f7f6567b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,os.path\n",
    "#sys.path.append(\"../../\")   # cite IC from parent directory\n",
    "sys.path.append(\"/gluster/data/next/software/IC_satkill/\")\n",
    "#sys.path.append(\"/gluster/data/next/software/IC_sophronia/\")\n",
    "#sys.path.append(os.path.expanduser('~/code/eol_hsrl_python'))\n",
    "sys.path.append(os.path.expanduser('~/code/eol_hsrl_python'))\n",
    "os.environ['ICTDIR']='/gluster/data/next/software/IC_satkill/'\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import tables as tb\n",
    "import IC.invisible_cities.io.dst_io                           as     dstio\n",
    "import IC.invisible_cities.io.mcinfo_io as mcio\n",
    "from    IC.invisible_cities.core.core_functions   import shift_to_bin_centers\n",
    "#import iminuit,probfit\n",
    "\n",
    "import scipy.special as special\n",
    "from scipy.stats import skewnorm\n",
    "from scipy.optimize import curve_fit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d558e310-5db7-4a03-bd51-ef3a34546e35",
   "metadata": {},
   "source": [
    "### importing all the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99aba869-d08e-4eb6-a24b-a7144f03b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###########################################################################################\n",
    "###########################################################################################\n",
    "######### DEFINE FUNCTIONS BELOW:\n",
    "###########################################################################################\n",
    "###########################################################################################\n",
    "\n",
    "def plot_hist(df, column = 'energy', binning = 20, title = \"Energy plot\", output = False, fill = True, label = 'default', x_label = 'energy (MeV)', range = 0, log = True, data = False, save = False, save_dir = ''):\n",
    "    '''\n",
    "    Print a histogram of energy from our dataframe,.\n",
    "    '''\n",
    "    # for simplicity/readability, scoop out the relevant columns from the pandas dataframe.\n",
    "    energy_vals = df[column].to_numpy()\n",
    "\n",
    "    if (range==0):\n",
    "        range = (np.min(energy_vals), np.max(energy_vals))\n",
    "\n",
    "    # control viewing of hist\n",
    "    if (fill == True):\n",
    "        cnts, edges, patches = plt.hist(energy_vals, bins = binning, label = label, range = range)\n",
    "    else:\n",
    "        cnts, edges, patches = plt.hist(energy_vals, bins = binning, label = label, histtype='step', linewidth = 2, range = range)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"events\")\n",
    "    plt.xlabel(x_label)\n",
    "    if log == True:\n",
    "        plt.yscale('log')\n",
    "    if (save==True):\n",
    "        if not (save_dir == ''):\n",
    "            plt.savefig(save_dir + title + \".png\")\n",
    "        else:\n",
    "            print(\"Please provide a suitable directory to save the data!\")\n",
    "    if (output==True):\n",
    "        plt.show()\n",
    "    if (data==True):\n",
    "        return (cnts, edges, patches)\n",
    "    else:\n",
    "        return\n",
    "\n",
    "\n",
    "def load_data(folder_path):\n",
    "    file_names = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f)) and f.endswith('.h5')]\n",
    "    \n",
    "    # remove any files that dont end in h5\n",
    "\n",
    "    # NOTE Break this section up, its annoying like this.\n",
    "    dfs = []\n",
    "    df_trs = []\n",
    "    df_ems = []\n",
    "    i = 0\n",
    "    end = len(file_names)\n",
    "    # create massive dataframe with all of them\n",
    "    for file in file_names:\n",
    "        file_path = folder_path + file\n",
    "        df = dstio.load_dst(file_path, 'Tracking', 'Tracks')\n",
    "        dfs.append(df)\n",
    "        # include MC particles (boooo takes ages)\n",
    "\n",
    "        # collecting the correct components of the file, not exactly sure how this works\n",
    "        df_ps = pd.read_hdf(file_path, 'MC/particles')\n",
    "        #df_ps = df_ps[df_ps.creator_proc == 'conv']\n",
    "        # collecting event map\n",
    "        df_em = mcio.load_eventnumbermap(file_path).set_index('nexus_evt')\n",
    "        df_trs.append(df_ps)\n",
    "        df_ems.append(df_em)\n",
    "        i += 1\n",
    "\n",
    "        if (i%50 == 0):\n",
    "            print(i)\n",
    "\n",
    "    tracks = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "    particles = pd.concat(df_trs, ignore_index=True)\n",
    "    particles['event_id'] = particles['event_id'] * 2   # double it\n",
    "\n",
    "    eventmap = pd.concat([dt for dt in df_ems])\n",
    "    # create particle list also\n",
    "\n",
    "    return (tracks, particles, eventmap)\n",
    "\n",
    "def cut_effic(df1, df2, verbose = False):\n",
    "    '''\n",
    "    Prints efficiency of cuts for singular cut\n",
    "    df1 -> cut df\n",
    "    df2 -> initial df\n",
    "    '''\n",
    "    length_1 = df1['event'].nunique()\n",
    "    length_2 = df2['event'].nunique()\n",
    "    efficiency = ((length_1/length_2)*100)\n",
    "    print(\"Efficiency: {:.2f} %\".format(efficiency))\n",
    "\n",
    "    if (verbose == True):\n",
    "        print((\"Events in reduced dataframe: {}\\nEvents in initial dataframe: {}\").format(len(df1), len(df2)))\n",
    "\n",
    "    return efficiency\n",
    "\n",
    "\n",
    "def fiducial_track_cut_2(df, lower_z = 20, upper_z = 1195, r_lim = 472, verbose = False):\n",
    "    '''\n",
    "    Produces fiducial track cuts while removing all events that have outer fiducial tracks\n",
    "    '''\n",
    "    # create lists of outer_fiduc entries\n",
    "    z_df_low = df[(df['z_min'] <= lower_z)]\n",
    "    z_df_up = df[(df['z_max'] >= upper_z)]\n",
    "    r_df = df[(df['r_max'] >= r_lim)]\n",
    "\n",
    "    # scrape the events\n",
    "    low_list = (z_df_low['event'].to_numpy())\n",
    "    up_list = (z_df_up['event'].to_numpy())\n",
    "    r_list = (r_df['event'].to_numpy())\n",
    "\n",
    "    # apply the filter to remove all events that fall in outer fiduc\n",
    "    df1 = df[~df['event'].isin(low_list)]\n",
    "    df2 = df1[~df1['event'].isin(up_list)]\n",
    "    df3 = df2[~df2['event'].isin(r_list)]\n",
    "\n",
    "    if (verbose == True):\n",
    "        print(\"Cutting events around fiducial volume related to:\\nZ range between {} and {}\\nRadius range < {}\".format(lower_z, upper_z, r_lim))\n",
    "\n",
    "\n",
    "    return df3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def one_track_cuts(df, verbose = False):\n",
    "    '''\n",
    "    Remove events with more than one track\n",
    "    THERE IS A COLUMN WITH THIS INFO IN IT, CALCULATING IT IS UNNECESSARY\n",
    "    '''\n",
    "    # 1-track event counter\n",
    "    event_counts = df.groupby('event').size()\n",
    "    #print(event_counts[:5]) # showing that you see how many \n",
    "                            #  trackIDs there are per event\n",
    "    one_track = event_counts[event_counts == 1].index\n",
    "\n",
    "    # filter dataframe\n",
    "    one_track_events = df[df['event'].isin(one_track)]\n",
    "    \n",
    "\n",
    "    if (verbose == True):\n",
    "        print(\"Removing events with more than one track.\")\n",
    "        print(\"Events with one track: {}\".format(one_track))\n",
    "        display(one_track_events.head())\n",
    "    \n",
    "\n",
    "    return one_track_events\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def overlapping_cuts(df, verbose = False):\n",
    "    '''\n",
    "    Remove all events with energy overlap != 0\n",
    "    '''\n",
    "\n",
    "    ovlp_remove = df[df['ovlp_blob_energy']==0]\n",
    "\n",
    "    if (verbose==True):\n",
    "        print(\"Removing overlapping blobs...\")\n",
    "\n",
    "    return ovlp_remove\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def energy_cuts(df, lower_e = 1.5, upper_e = 1.7, verbose = False):\n",
    "    '''\n",
    "    Apply cuts around the relevant energy\n",
    "    '''\n",
    "    filt_e_df = df[(df['energy'] >= lower_e) & (df['energy'] <= upper_e)]\n",
    "\n",
    "    if (verbose == True):\n",
    "        print(\"Cutting energy events around {} & {} keV\".format(lower_e, upper_e))\n",
    "\n",
    "    return filt_e_df\n",
    "\n",
    "\n",
    "def remove_low_E_events(df, energy_limit = 0.05):\n",
    "    '''\n",
    "    Remove low energy tracks, add their energy back to the first\n",
    "    track and then update 'numb_of_tracks' to be up to date\n",
    "    '''\n",
    "\n",
    "    tracks_test = df.copy(deep=True)\n",
    "\n",
    "    # take events with lower than 50 keV, 0.05 MeV\n",
    "    condition = (tracks_test.energy < energy_limit)\n",
    "    summed_df = tracks_test[condition].groupby('event')['energy'].sum().reset_index()\n",
    "\n",
    "     # merge these as a new column\n",
    "    merged_df = pd.merge(tracks_test, summed_df, on='event', suffixes=('', '_sum'), how = 'left').fillna(0)\n",
    "    # add this summed energy to first column\n",
    "    merged_df['energy'] = merged_df.apply(lambda row: (row['energy'] + row['energy_sum']) if row.name == merged_df[merged_df['event'] == row['event']].index[0] else row['energy'], axis=1)\n",
    "\n",
    "    # drop energy sum column\n",
    "    result_df = merged_df.drop('energy_sum', axis = 1)\n",
    "\n",
    "    # then remove all tracks below the energy threshold\n",
    "    condition_upper = (result_df.energy > energy_limit)\n",
    "    remove_low_E = result_df[condition_upper]\n",
    "\n",
    "    # count the number of events identified with unique event, and change numb_of_tracks to reflect this\n",
    "    event_counts = remove_low_E['event'].value_counts(sort = False)\n",
    "\n",
    "    # apply this to numb_of_tracks\n",
    "    remove_low_E['numb_of_tracks'] = remove_low_E['event'].map(event_counts)\n",
    "\n",
    "    return remove_low_E\n",
    "\n",
    "\n",
    "def len_events(df):\n",
    "    '''\n",
    "    Returns the number of unique events as len(df) doesn't work in this case\n",
    "    '''\n",
    "    length_1 = df['event'].nunique()\n",
    "    return length_1\n",
    "\n",
    "\n",
    "def positron_scraper(data_path, save = False):\n",
    "    \"\"\"\n",
    "    Function that iterates over files with MC and collects only positron events.\n",
    "    Intended to reduce the memory resources of MC data.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "     # collect all filenames\n",
    "    try:\n",
    "        file_names = [f for f in os.listdir(data_path) if os.path.isfile(os.path.join(data_path, f)) and f.endswith('.h5')]\n",
    "    except:\n",
    "        print(\"File path incorrect, please state the correct file path\\n(but not any particular folder!)\")\n",
    "\n",
    "\n",
    "    # read in a singular file to collect the column titles\n",
    "    \n",
    "    MC_df_single = pd.read_hdf(data_path + file_names[0], 'MC/particles')\n",
    "\n",
    "    MC_df = []\n",
    "    pos_df = pd.DataFrame(columns = MC_df_single.columns)\n",
    "    eventmap = []\n",
    "\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "\n",
    "    # how much you chunk your data\n",
    "    chunker = np.floor(len(file_names)*0.1)\n",
    "\n",
    "    # chunk file_names\n",
    "    \n",
    "    for file in file_names:\n",
    "        file_path = data_path + file\n",
    "\n",
    "        # load in file\n",
    "        MC_df_temp = pd.read_hdf(file_path, 'MC/particles')\n",
    "        MC_df.append(MC_df_temp)\n",
    "        eventmap.append(mcio.load_eventnumbermap(file_path).set_index('nexus_evt'))\n",
    "\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        # chunk checker, every time you hit a certain chunk,\n",
    "        # collect the positron events and wipe the df\n",
    "        if ((i%chunker) == 0):\n",
    "            #print(\"Chunking at event {}!\".format(i))\n",
    "            # concat the list\n",
    "            MC_df = pd.concat(MC_df, axis = 0, ignore_index = True)\n",
    "            #print(\"Post concat\")\n",
    "            #display(MC_df)\n",
    "            pos_data = MC_df[MC_df['particle_name'] == 'e+']\n",
    "\n",
    "            \n",
    "            #display(pos_data)\n",
    "            #print(type(pos_data))\n",
    "            # collect positron events into df\n",
    "            pos_df = pos_df.append(pos_data)\n",
    "            #print(\"{} positron events found\\n{} positron events total\".format(pos_data.shape[0],pos_df.shape[0]))\n",
    "            #display(pos_df)\n",
    "\n",
    "            # make space\n",
    "            MC_df = []\n",
    "\n",
    "    if (save == True):\n",
    "        pos_df.to_hdf('positrons.h5', key = 'pos', mode = 'w')\n",
    "\n",
    "    return pos_df\n",
    "\n",
    "\n",
    "def blob_positron_plot(ecut_rel, ecut_no_positron_df, save = False, save_title = 'plot.png'):\n",
    "    '''\n",
    "    Plots the blob energies with and without positrons.\n",
    "    '''\n",
    "\n",
    "    # the original way\n",
    "    plot_hist(ecut_rel, column = 'eblob2', binning = 20, title = \"Blob energies\", output = False, fill = False, label = 'blob 2', x_label = 'energy (MeV)', range = (minimum_e, maximum_e))\n",
    "    plot_hist(ecut_rel, column = 'eblob1', binning = 20, title = \"Blob energies\", output = False, fill = False, label = 'blob 1', x_label = 'energy (MeV)', range = (minimum_e, maximum_e))\n",
    "\n",
    "    #plt.hist(no_pos_blob1, bins = 20, label = 'events with no e+', range = (minimum_e, maximum_e))\n",
    "    #plt.hist(no_pos_blob2, bins = 20, label = 'events with no e+', range = (minimum_e, maximum_e))\n",
    "\n",
    "    plot_hist(ecut_no_positron_df, column = 'eblob1', binning = 20, title = \"Blob energies\", output = False, fill = True, label = '- events with no e+', x_label = 'energy (MeV)', range = (minimum_e, maximum_e))\n",
    "    plot_hist(ecut_no_positron_df, column = 'eblob2', binning = 20, title = \"Blob energies\", output = False, fill = True, label = '- events with no e+', x_label = 'energy (MeV)', range = (minimum_e, maximum_e))\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    if (save == True):\n",
    "        plt.savefig(save_title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def true_fom_calc(p_data, no_p_data, cut_list, verbose = False):\n",
    "    '''\n",
    "    produces a figure of merit list based\n",
    "    on cuts to specific categories and their\n",
    "    consequent fits\n",
    "\n",
    "    '''\n",
    "\n",
    "    # create deep copies for safety\n",
    "    pos_data = p_data.copy(deep = True)\n",
    "    no_pos_data = no_p_data.copy(deep = True)\n",
    "\n",
    "    if (verbose == True):\n",
    "        blob_positron_plot(pos_data, no_pos_data)\n",
    "    # Take the initial, no blob2 cut values for ns and nb\n",
    "    ns0 = len(pos_data.index)\n",
    "    nb0 = len(no_pos_data.index)\n",
    "\n",
    "    # create all the lists for fom\n",
    "    e = []\n",
    "    b = []\n",
    "    fom = []\n",
    "\n",
    "    for i in range(len(cut_list)):\n",
    "        \n",
    "        # remove blob 2 values below value on cut_list\n",
    "        pos_data = pos_data[(pos_data['eblob2'] > cut_list[i])]\n",
    "        no_pos_data = no_pos_data[(no_pos_data['eblob2'] > cut_list[i])]\n",
    "\n",
    "\n",
    "        # apply fit to the new data \n",
    "        if (verbose == True):\n",
    "            print(\"Signal events: {}\\nBackground events: {}\\n FOM: {}\".format())\n",
    "        \n",
    "        # collect number of signal events vs number of backgrounds, which you know \n",
    "        ns = len(pos_data.index)\n",
    "        nb = len(no_pos_data.index)\n",
    "\n",
    "\n",
    "\n",
    "        # produce fom value, if ns0 or nb is zero, set to zero.\n",
    "        try:\n",
    "            e.append(ns/ns0)\n",
    "        except:\n",
    "            print(\"Zero-div error, appending 0\")\n",
    "            e.append(0)\n",
    "        \n",
    "        try:\n",
    "            b.append(nb/nb0)\n",
    "        except ZeroDivisionError:\n",
    "            print(\"Zero-div error, appending 0\")\n",
    "            b.append(0)\n",
    "        fom.append(e[i]/np.sqrt(b[i]))\n",
    "\n",
    "        if (verbose == True):\n",
    "            blob_positron_plot(pos_data, no_pos_data)\n",
    "        \n",
    "    # that should be it? i dont expect this to work first time, but lets test it!\n",
    "    return fom\n",
    "\n",
    "###########################################################################################\n",
    "###########################################################################################\n",
    "######### FUNCTIONS END\n",
    "###########################################################################################\n",
    "###########################################################################################\n",
    "\n",
    "def process_data(folder_path):\n",
    "\n",
    "    print(\"Opening files...\")\n",
    "    # load and unpack data, assume you're sitting in the PORT_XX folder\n",
    "    data = load_data(str(folder_path) + 'isaura/') \n",
    "    tracks = data[0]\n",
    "    particles = data[1]\n",
    "    eventmap = data[2]\n",
    "\n",
    "\n",
    "    # save raw histogram\n",
    "    plot_hist(tracks, column = 'energy', output= False, binning = 65, title = \"raw_hist\",\n",
    "            fill = True, data = False, save = True, save_dir = str(folder_path) + 'output/')\n",
    "\n",
    "\n",
    "    print(\"Applying Cuts...\")\n",
    "\n",
    "    # remove low energy satellites first\n",
    "    \n",
    "    # apply cuts, SATELLITE TRACKS ARE DIFFERENT RN\n",
    "    #low_e_tracks = func.remove_low_E_events(full_tracks)\n",
    "    low_e_cut_tracks = tracks[tracks.energy > 0.05]\n",
    "    # count the number of events identified with unique event, and change numb_of_tracks to reflect this\n",
    "    event_counts = low_e_tracks['event'].value_counts(sort = False)\n",
    "\n",
    "    # apply this to numb_of_tracks\n",
    "    low_e_cut_tracks['numb_of_tracks'] = low_e_tracks['event'].map(event_counts)\n",
    "\n",
    "\n",
    "\n",
    "    # Efficiency calculation\n",
    "    cut_names = []\n",
    "    rel_cut_effics = []\n",
    "    abs_cut_effics = []\n",
    "    cut_events = []\n",
    "\n",
    "    # no cuts\n",
    "    cut_names.append(\"No cuts\")\n",
    "    rel_cut_effics.append(100)\n",
    "    abs_cut_effics.append(100)\n",
    "    # number of events\n",
    "    cut_events.append(len_events(tracks))\n",
    "\n",
    "\n",
    "    #####################################################################\n",
    "    #####################################################################\n",
    "\n",
    "    # fiducial cuts\n",
    "    cut_names.append(\"Fiducial Cuts\")\n",
    "\n",
    "    # make fiducial cuts\n",
    "    fiducial_rel = fiducial_track_cut_2(low_e_cut_tracks, lower_z = 20, upper_z=1170, r_lim = 415, verbose = False)\n",
    "\n",
    "    fiducial_abs = fiducial_track_cut_2(tracks, lower_z = 20, upper_z=1170, r_lim = 415, verbose = True)\n",
    "\n",
    "    # make efficiency calculation\n",
    "    print(\"Fiducial track cut\")\n",
    "    print(\"==================\")\n",
    "    print(\"Relative Cut efficiency:\")\n",
    "    ef = cut_effic(fiducial_rel, low_e_cut_tracks)\n",
    "    rel_cut_effics.append(ef)\n",
    "    cut_events.append(len_events(fiducial_rel))\n",
    "\n",
    "    print('Absolute Cut efficiency:')\n",
    "    ef = cut_effic(fiducial_abs, tracks)\n",
    "    abs_cut_effics.append(ef)\n",
    "\n",
    "\n",
    "\n",
    "    #####################################################################\n",
    "    #####################################################################\n",
    "\n",
    "    cut_names.append(\"One track cut\")\n",
    "    one_track_rel = one_track_cuts(fiducial_rel, verbose = False)\n",
    "\n",
    "    # events are relative, as absolute efficiency lets you figure out events from the beginning# absolute\n",
    "    one_track_abs = one_track_cuts(tracks)\n",
    "\n",
    "    # relative\n",
    "    print(\"One track cut\")\n",
    "    print(\"================\")\n",
    "    print(\"Relative Cut efficiency:\")\n",
    "    ef = cut_effic(one_track_rel, fiducial_rel)\n",
    "    rel_cut_effics.append(ef)\n",
    "    cut_events.append(len_events(one_track_rel))\n",
    "\n",
    "    # absolute\n",
    "    print(\"Absolute Cut efficiency:\")\n",
    "    ef = cut_effic(one_track_abs, tracks)\n",
    "    abs_cut_effics.append(ef)\n",
    "\n",
    "\n",
    "\n",
    "    #####################################################################\n",
    "    #####################################################################\n",
    "\n",
    "    # apply cuts\n",
    "    ovlp_rel = overlapping_cuts(one_track_rel)\n",
    "    ovlp_abs = overlapping_cuts(tracks)\n",
    "\n",
    "\n",
    "    cut_names.append(\"Blob overlap cuts\")\n",
    "\n",
    "    # relative\n",
    "    print(\"Blob overlap cut\")\n",
    "    print(\"================\")\n",
    "    print(\"Relative Cut efficiency:\")\n",
    "    ef = cut_effic(ovlp_rel, one_track_rel)\n",
    "    rel_cut_effics.append(ef)\n",
    "    cut_events.append(len_events(ovlp_rel))\n",
    "\n",
    "\n",
    "    # absolute\n",
    "    print(\"Absolute Cut efficiency:\")\n",
    "    ef = cut_effic(ovlp_abs, tracks)\n",
    "    abs_cut_effics.append(ef)\n",
    "\n",
    "\n",
    "    #####################################################################\n",
    "    #####################################################################\n",
    "\n",
    "    ecut_rel = energy_cuts(ovlp_rel)\n",
    "    ecut_abs = energy_cuts(tracks)\n",
    "\n",
    "    cut_names.append(\"Energy cuts\")\n",
    "\n",
    "    # relative\n",
    "    print(\"Energy cut\")\n",
    "    print(\"================\")\n",
    "    print(\"Relative Cut efficiency:\")\n",
    "    ef = cut_effic(ecut_rel, ovlp_rel)\n",
    "    rel_cut_effics.append(ef)\n",
    "    cut_events.append(len_events(ecut_rel))\n",
    "\n",
    "\n",
    "    # absolute\n",
    "    print(\"Absolute Cut efficiency:\")\n",
    "    ef = cut_effic(ecut_abs, tracks)\n",
    "    abs_cut_effics.append(ef)\n",
    "\n",
    "\n",
    "    efficiencies = pd.DataFrame({'Cut': cut_names,\n",
    "                             'Relative Efficiency': rel_cut_effics,\n",
    "                             'Relative Events': cut_events,\n",
    "                             'Single Cut Efficiency': abs_cut_effics\n",
    "                             })\n",
    "\n",
    "\n",
    "    # adding exception in for when there's no data in ecut_rel\n",
    "    if (len(ecut_rel.index) == 0):\n",
    "            efficiencies.loc[len(efficiencies.index)] = ['pos_evt - all_evt', 0, len(ecut_rel), 0]\n",
    "            efficiencies.loc[len(efficiencies.index)] = ['FOM_MAX - blob2_E_val (MeV)', 0, 0, 0]\n",
    "            efficiencies.loc[len(efficiencies.index)] = ['trk_no - satellite_no', len(tracks.index), len(tracks.index) - len(low_e_cut_tracks.index), 0]\n",
    "            efficiencies.to_csv(str(folder_path) + 'output/efficiency.csv')\n",
    "            print(\"No events left in ROI... jobs done!\")\n",
    "            return 0\n",
    "\n",
    "            \n",
    "        \n",
    "    plot_hist(ecut_rel, column = 'energy', output= False, binning = 20, title = \"cut_hist\",\n",
    "                fill = True, data = False, save = True, save_dir = str(folder_path) + 'output/', log = False)\n",
    "\n",
    "\n",
    "    ###########################################################################################\n",
    "    # EFFICIENCY CALCULATION OVER\n",
    "    ###########################################################################################\n",
    "\n",
    "    print(\"Calculating FOM\")\n",
    "\n",
    "    # collect positron events\n",
    "    positron_events = positron_scraper(str(folder_path) + 'isaura/')\n",
    "    pos_events = (np.unique(positron_events['event_id'].to_numpy()))*2\n",
    "\n",
    "    # number of events that are positrons\n",
    "    ecut_positron_df = ecut_rel[ecut_rel['event'].isin(pos_events)]\n",
    "    ecut_no_positron_df = ecut_rel[~ecut_rel['event'].isin(pos_events)]\n",
    "    cut_list = np.linspace(0,0.6,61)\n",
    "    fom = true_fom_calc(ecut_positron_df, ecut_no_positron_df, cut_list)\n",
    "    # sanitise\n",
    "    fom = np.nan_to_num(fom)\n",
    "\n",
    "    print(\"FOM values:\")\n",
    "    print(fom)\n",
    "\n",
    "    # remove stupid values based on low statistics\n",
    "    fom[fom > 10] = 0\n",
    "    fom[fom < 0] = 0\n",
    "\n",
    "    max_index = np.argmax(fom)\n",
    "\n",
    "\n",
    "    efficiencies.loc[len(efficiencies.index)] = ['pos_evt - all_evt', len(ecut_positron_df), len(ecut_rel), 0]\n",
    "    efficiencies.loc[len(efficiencies.index)] = ['FOM_MAX - blob2_E_val (MeV)', fom[max_index], cut_list[max_index], 0]\n",
    "    efficiencies.loc[len(efficiencies.index)] = ['trk_no - satellite_no', len(tracks.index), len(tracks.index) - len(low_e_cut_tracks.index), 0]\n",
    "    \n",
    "    efficiencies.to_csv(str(folder_path) + 'output/efficiency.csv')\n",
    "\n",
    "    print(\"Jobs done!\")\n",
    "\n",
    "    # Save the data to a h5 file\n",
    "    ecut_rel.to_hdf(str(folder_path) + 'output/post_cuts.h5', key='cut_data', mode = 'w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e577c180-542d-4e95-9ecb-583610127105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening files...\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n"
     ]
    }
   ],
   "source": [
    "process_data('/gluster/data/next/files/TOPOLOGY_John/HPR_PARAMETER_CHECK/data/PORT_1a/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d2fca2-7a67-4d32-a092-19434f9e8c53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
